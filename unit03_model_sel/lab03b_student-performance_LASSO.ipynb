{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab:  Feature Selection for Linear Regression for Student Performance Data  \n",
    "\n",
    "In this lab we use the UCI dataset of `Student Performance` to use linear regression with LASSO regularization. We will also look at some Feature Selection methods. The dataset is about student achievement in secondary education of two Portuguese schools. The target variable is the student's grade in their Mathematics exam and there are many features such as  demographic (address), social (family, age, sex, etc) and school related (schoolName, study time etc) features. So, we will try to predict the student's grades based on their background.\n",
    "\n",
    "This lab has the following objectives\n",
    "\n",
    "1. Learn about converting the categorical dataset to numerical values.\n",
    "2. Perform LASSO regression and compare the results with simple linear regression.\n",
    "3. Visualize the features obtained by LASSO and the LASSO path.\n",
    "4. Learn another technique for feature selection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the data\n",
    "The dataset is available at \n",
    "<a href = \"https://archive.ics.uci.edu/ml/datasets/Student+Performance\">\n",
    "P. Cortez and A. Silva. Using Data Mining to Predict Secondary School Student Performance. In A. Brito and J. Teixeira Eds., Proceedings of 5th FUture BUsiness TEChnology Conference (FUBUTEC 2008) pp. 5-12, Porto, Portugal, April, 2008, EUROSIS, ISBN 978-9077381-39-7. </a>\n",
    "\n",
    "You need to download the Data Folder which is a `student.zip` file. It contains `student-mat.csv` file which we will use in this lab. You should go through the website to understand the meaning of each feature in the dataset, to be able to interpret your results.\n",
    "\n",
    "We start with loading the basic packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib \n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, use `pd.read_csv(...)` to load the `student-mat.csv` file. Also, print the first 6 samples of dataframe `df`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#TODO\n",
    "#df = pd.read_csv(...)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that the dataset contains a mixture of numerical and categorial features. For our analysis we can convert the categories to a numerical value. We can use two techniques-\n",
    "\n",
    "1. **One-Hot Coding**: Create K new binary features for each categorical feature with K categories.\n",
    "2. **Label Encoder**: Map categorical values of a feature to numericals using whole numbers (0,1,2,...).\n",
    "\n",
    "We first look at the datatype of each features. Use the command `df.dtypes` and display the results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some of the features are of datatype **object**. Use the `select_dtypes` method in Pandas DataFrame to identify the categorical features (features of datatype `object`) and save the name of those features into a list `categorical_features`. Print this list. You should get these set of features: ['school', 'sex', 'address', 'famsize', 'Pstatus', 'Mjob', 'Fjob', 'reason', 'guardian', 'schoolsup', 'famsup', 'paid', 'activities', 'nursery', 'higher', 'internet', 'romantic']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#TODO\n",
    "\n",
    "#categorical_features =\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One-Hot Coding and Label Encoder\n",
    "For a categorical feature with more than two categories, we should use one-hot-coding (OHC) to convert it to binary features. However, for a categorical feature with only two categories, we should apply Label Encoder.  We first list the features that need OHC and those that need Label Encoder.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ohc_category = ['Mjob','Fjob','reason','guardian']\n",
    "le_category = ['school', 'sex', 'address', 'famsize', 'Pstatus', 'schoolsup', 'famsup', 'paid', 'activities',\n",
    "       'nursery', 'higher', 'internet', 'romantic']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One-Hot Coding\n",
    "We first use One-Hot Coding to all categorical features. Pandas has a method called `get_dummies()` to do the job. It's interesting that this method is called `get_dummies` because it generates new dummy features corresponding to each categories. Find a new dataframe df_ohc which replace those features in the ohc_category by one-hot coding (apply get_dummies on columns in ohc_category). Also print the first 6 samples of the new dataframe `df_ohc`, and observe and comment on how each categorical feature is converted to multiple binary numerical features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#TODO\n",
    "#df_ohc =\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Encoder\n",
    "Now we further convert those in the `df_ohc` data frame that are in the `le_category` using Label Encoder. Find a new dataframe `df_le` which is a copy of dataframe `df_ohc` except that all the binary categorial features are encoded to a numerical value of 0 or 1. You should use the `fit_transform()` method of the `LabelEncoder()`. Print first 6 lines of `df_le`, and make sure the entries in the final data frame are all properly encoded into numerical features.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "#TODO\n",
    "\n",
    "# df_le = df_ohc.copy()\n",
    "\n",
    "# Hint: Now use a for loop over the elements in `le_category` and update df_le"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset has three targets namely G1, G2, and G3 which represents the grades in midterm1, midterm2 and final exams respectively. These variables are highly correlated with each other and therefore, if we use G3 as out target, it is not interesting to include G1 and G2 to our features. For our exercise, we will drop G1,G2, and G3 from the feature list, and use G1 as the target. You could try to use G2 or G3 as the target as well and see what happens, but submit the results with target 'G1' only.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_df = df_le.drop(['G1','G2','G3'],axis=1)\n",
    "col_names = X_df.columns\n",
    "X = np.array(X_df)\n",
    "y = np.array(df_le['G1'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If there are `nsamples` number of samples and `nfeatures` number of features, use the `shape` method to find them and print their values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Linear Regression\n",
    "\n",
    "Train a linear model using half of the samples and test the trained model using the other half samples. Print the Normalized train and test RSS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "\n",
    "# TODO\n",
    "# ns_train = nsamples // 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should observe that the normalized training RSS is reasonably small, but the testing error is large! One way to understand why this is the case is by printing the coeffcient values. Use the\n",
    "`coef_` method of model `regr` to get the regression coefficients. Print the coefficients in decreasing order of their magnitudes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#TODO\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question:\n",
    "Can you explain in one or two sentences below why the linear regression gives a large test error?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Type Answer Here:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using LASSO regression\n",
    "\n",
    "Now let us try to use LASSO regression to select the optimal features. Note that it is extremely important to normalize (standardize) the data for the regularization methods. \n",
    "\n",
    "First use the `preprocessing.scale` method to standardize the data matrix `X` and target `y`.  Store the standardized values in `Xs` and `ys` respectively.    For this data, the `scale` routine may throw a warning that you are converting data types.  That is fine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "# TODO\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, use the LASSO method to fit a model.  Use cross validation to select the regularization level `alpha`.  Use 100 `alpha` values logarithmically spaced from `1e-3` to `10`, and use 10 fold cross validation.  Store the test RSS in a matrix `RSSts` with rows correponding to different `alpha` values and columns corresponding to different cross validation folds. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import  sklearn.model_selection \n",
    "\n",
    "#TODO\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Determine the RSS mean and standard error corresponding to different alpha and plot the mean RSS with error bar as a function of alpha. Label the axis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO\n",
    "\n",
    "# RSS_mean = \n",
    "# RSS_std = \n",
    "# plt.errorbar(...)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find the optimal `alpha` and the mean test RSS at this optimal point using the one standard error rule."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TO DO\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LASSO path\n",
    "\n",
    "To further illustrate the effect of regularization, we conclude by drawing the *LASSO path*.  This is simply a plot of the coefficients as a function of the regularization `alpha`. The path demonstrates the effect of regularization well. Use the `lasso_path` method to obtain all the coefficients for the given range of alphas and plot the LASSO path. Also draw a vertical line at optimal value of alpha."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Non-zero LASSO Coefficients\n",
    "Using the above coefficients, Plot the number of non-zero coefficients vs alpha. Also draw a vertical line at optimal value of alpha. You can assume any coefficients with magnitude <= 0.001 to be zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now find out with this optimal alpha, what coefficients are nonzero. Let us consider any coefficients with absoluate value <= 0.001 as zero. You need to first do a model fit using the entire data (`Xs`, `ys`) to find the model coefficients. Then determine and **print** the number of nonzeros (Call it `nfea1`). Finally, **print** the corresponding feature name (Hint: use the list `col_names` obtained previously) with their corresponding coefficient in the order with decreasing coefficient magnitudes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#TO DO\n",
    "\n",
    "#First do a model fit using alpha_opt\n",
    "\n",
    "# model.alpha = alpha_opt\n",
    "# model.fit(Xs, ys)\n",
    "\n",
    "# Find model coefficients that are >0.001 and count\n",
    "# nfea1 = \n",
    "\n",
    "\n",
    "# Sort the coeffients in decreasing order and print. \n",
    "# sorted_coeff = np.argsort( ... )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question:\n",
    "Observe the coefficient values carefully. Do they make sense to you? Do you see positive coefficients for features that you think  are likely to increase student performance, and vice verse? Put your answer below. You may need to consult the original datasource, to understand the meaning of each chosen feature.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Type answer here:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Determine linear predictor with the selected features using the optimal alpha\n",
    "\n",
    "Note that we cannot use the coefficients determined above as is as our predictor, as it is fitted using all data. Also it obtained by minimizing the LASSO loss. With the selected features, we now go through a 10 fold cross validation to determine the predictor coefficients and the estimated test error using the **linear regression** directly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First get new feature array only containing these chosen features and name it *X_new*. Print a few lines to make sure that you got the data correctly.\n",
    "\n",
    "Note: You can convert the dataframe into numpy array using `np.array(...)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TO DO\n",
    "# X_new = \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now compute the mean predictor coefficients and mean test error using 10 fold cross validation. For this part, you can use the linear predictor on the original features (not scaled) directly, or using the scaled data. Using the scaled data enables you to judge the importance of featurs based on the coefficient magnitude. Therefore let us use scaled data. \n",
    "\n",
    "Store the mean coefficients in `coeff_mean` and the mean intercept in `intercept_mean`. Print them along with the feature names. Also print the mean and std of RSS.\n",
    "\n",
    "Hint: Get the intercept using `regr.intercept_` attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Scale the data\n",
    "#Xs_new = preprocessing.scale(X_new)\n",
    "\n",
    "# TODO\n",
    "# 10 fold CV using the linear regression model to find the RSS and coef using each fold \n",
    "\n",
    "# Hint: First set up arrays to store the test errors and coefficients then go through a loop of 10 folds\n",
    "\n",
    "# RSSts = np.zeros((nfold,1))\n",
    "# coef=np.zeros((nfold,nfea1))\n",
    "# intercept=np.zeros((nfold,1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the **mean predictor** above, compute the predicted response variable on the whole data (`Xs_new`). Plot the predicted vs. actual values. Also show a line of 45 degree slope. Also print the normalized RSS.\n",
    "\n",
    "Hint: Use $\\hat{y} = Xs_{new}\\beta+\\beta_0$, where $\\beta$ is the vector `coef_mean` and $\\beta_0$ is `intercept_mean`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TO DO\n",
    "# Compute the response and plot\n",
    "\n",
    "# yhat = \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Ranking based on F-test and mutual information \n",
    "\n",
    "Rank all the original features (`Xs`) using f-test metric. Print first *nfea1* top ranked feature names. If you wish, you could try other metrics as well (e.g. mutual information). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import f_regression\n",
    "\n",
    "# TO DO\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Error with f_test\n",
    "Take the top ranked *nfea1* features, apply linear regression fit in cross validation to find the test error, to see how they compare with LASSO result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TO DO\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question:\n",
    "Comment on the features choosen, their corresponding coefficient values, and the test error, in contrast to those obtained with LASSO. Explain why the feature ranking method is generally not as effective as LASSO."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Type answer here: \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test error with MI (Optional)\n",
    "Similarly obtain the test error with MI and compare with LASSO and f_test.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

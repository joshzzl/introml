\documentclass[11pt]{article}

\usepackage{fullpage}
\usepackage{amsmath, amssymb, bm, cite, epsfig, psfrag}
\usepackage{graphicx}
\usepackage{float}
\usepackage{amsthm}
\usepackage{amsfonts}
\usepackage{listings}
\usepackage{cite}
\usepackage{hyperref}
\usepackage{tikz}
\usepackage{enumerate}
\usepackage{listings}
\usepackage{mathtools}
\lstloadlanguages{Python}
\usetikzlibrary{shapes,arrows}
%\usetikzlibrary{dsp,chains}

\DeclareFixedFont{\ttb}{T1}{txtt}{bx}{n}{9} % for bold
\DeclareFixedFont{\ttm}{T1}{txtt}{m}{n}{9}  % for normal
% Defining colors
\usepackage{color}
\definecolor{deepblue}{rgb}{0,0,0.5}
\definecolor{deepred}{rgb}{0.6,0,0}
\definecolor{deepgreen}{rgb}{0,0.5,0}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}

%\restylefloat{figure}
%\theoremstyle{plain}      \newtheorem{theorem}{Theorem}
%\theoremstyle{definition} \newtheorem{definition}{Definition}

\def\del{\partial}
\def\ds{\displaystyle}
\def\ts{\textstyle}
\def\beq{\begin{equation}}
\def\eeq{\end{equation}}
\def\beqa{\begin{eqnarray}}
\def\eeqa{\end{eqnarray}}
\def\beqan{\begin{eqnarray*}}
\def\eeqan{\end{eqnarray*}}
\def\nn{\nonumber}
\def\binomial{\mathop{\mathrm{binomial}}}
\def\half{{\ts\frac{1}{2}}}
\def\Half{{\frac{1}{2}}}
\def\N{{\mathbb{N}}}
\def\Z{{\mathbb{Z}}}
\def\Q{{\mathbb{Q}}}
\def\R{{\mathbb{R}}}
\def\C{{\mathbb{C}}}
\def\argmin{\mathop{\mathrm{arg\,min}}}
\def\argmax{\mathop{\mathrm{arg\,max}}}
%\def\span{\mathop{\mathrm{span}}}
\def\diag{\mathop{\mathrm{diag}}}
\def\x{\times}
\def\limn{\lim_{n \rightarrow \infty}}
\def\liminfn{\liminf_{n \rightarrow \infty}}
\def\limsupn{\limsup_{n \rightarrow \infty}}
\def\GV{Guo and Verd{\'u}}
\def\MID{\,|\,}
\def\MIDD{\,;\,}

\newtheorem{proposition}{Proposition}
\newtheorem{definition}{Definition}
\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}
\newtheorem{corollary}{Corollary}
\newtheorem{assumption}{Assumption}
\newtheorem{claim}{Claim}
\def\qed{\mbox{} \hfill $\Box$}
\setlength{\unitlength}{1mm}

\def\bhat{\widehat{b}}
\def\ehat{\widehat{e}}
\def\phat{\widehat{p}}
\def\qhat{\widehat{q}}
\def\rhat{\widehat{r}}
\def\shat{\widehat{s}}
\def\uhat{\widehat{u}}
\def\ubar{\overline{u}}
\def\vhat{\widehat{v}}
\def\xhat{\widehat{x}}
\def\xbar{\overline{x}}
\def\zhat{\widehat{z}}
\def\zbar{\overline{z}}
\def\la{\leftarrow}
\def\ra{\rightarrow}
\def\MSE{\mbox{\small \sffamily MSE}}
\def\SNR{\mbox{\small \sffamily SNR}}
\def\SINR{\mbox{\small \sffamily SINR}}
\def\arr{\rightarrow}
\def\Exp{\mathbb{E}}
\def\var{\mbox{var}}
\def\Tr{\mbox{Tr}}
\def\tm1{t\! - \! 1}
\def\tp1{t\! + \! 1}
\def\Tm1{T\! - \! 1}
\def\Tp1{T\! + \! 1}


\def\Xset{{\cal X}}

\newcommand{\one}{\mathbf{1}}
\newcommand{\abf}{\mathbf{a}}
\newcommand{\bbf}{\mathbf{b}}
\newcommand{\dbf}{\mathbf{d}}
\newcommand{\ebf}{\mathbf{e}}
\newcommand{\gbf}{\mathbf{g}}
\newcommand{\hbf}{\mathbf{h}}
\newcommand{\pbf}{\mathbf{p}}
\newcommand{\pbfhat}{\widehat{\mathbf{p}}}
\newcommand{\qbf}{\mathbf{q}}
\newcommand{\qbfhat}{\widehat{\mathbf{q}}}
\newcommand{\rbf}{\mathbf{r}}
\newcommand{\rbfhat}{\widehat{\mathbf{r}}}
\newcommand{\sbf}{\mathbf{s}}
\newcommand{\sbfhat}{\widehat{\mathbf{s}}}
\newcommand{\ubf}{\mathbf{u}}
\newcommand{\ubfhat}{\widehat{\mathbf{u}}}
\newcommand{\utildebf}{\tilde{\mathbf{u}}}
\newcommand{\vbf}{\mathbf{v}}
\newcommand{\vbfhat}{\widehat{\mathbf{v}}}
\newcommand{\wbf}{\mathbf{w}}
\newcommand{\wbfhat}{\widehat{\mathbf{w}}}
\newcommand{\xbf}{\mathbf{x}}
\newcommand{\xbfhat}{\widehat{\mathbf{x}}}
\newcommand{\xbfbar}{\overline{\mathbf{x}}}
\newcommand{\ybf}{\mathbf{y}}
\newcommand{\zbf}{\mathbf{z}}
\newcommand{\zbfbar}{\overline{\mathbf{z}}}
\newcommand{\zbfhat}{\widehat{\mathbf{z}}}
\newcommand{\Ahat}{\widehat{A}}
\newcommand{\Abf}{\mathbf{A}}
\newcommand{\Bbf}{\mathbf{B}}
\newcommand{\Cbf}{\mathbf{C}}
\newcommand{\Bbfhat}{\widehat{\mathbf{B}}}
\newcommand{\Dbf}{\mathbf{D}}
\newcommand{\Gbf}{\mathbf{G}}
\newcommand{\Hbf}{\mathbf{H}}
\newcommand{\Ibf}{\mathbf{I}}
\newcommand{\Kbf}{\mathbf{K}}
\newcommand{\Pbf}{\mathbf{P}}
\newcommand{\Phat}{\widehat{P}}
\newcommand{\Qbf}{\mathbf{Q}}
\newcommand{\Rbf}{\mathbf{R}}
\newcommand{\Rhat}{\widehat{R}}
\newcommand{\Sbf}{\mathbf{S}}
\newcommand{\Ubf}{\mathbf{U}}
\newcommand{\Vbf}{\mathbf{V}}
\newcommand{\Wbf}{\mathbf{W}}
\newcommand{\Xhat}{\widehat{X}}
\newcommand{\Xbf}{\mathbf{X}}
\newcommand{\Ybf}{\mathbf{Y}}
\newcommand{\Zbf}{\mathbf{Z}}
\newcommand{\Zhat}{\widehat{Z}}
\newcommand{\Zbfhat}{\widehat{\mathbf{Z}}}
\def\alphabf{{\boldsymbol \alpha}}
\def\betabf{{\boldsymbol \beta}}
\def\betabfhat{{\widehat{\bm{\beta}}}}
\def\epsilonbf{{\boldsymbol \epsilon}}
\def\mubf{{\boldsymbol \mu}}
\def\lambdabf{{\boldsymbol \lambda}}
\def\etabf{{\boldsymbol \eta}}
\def\xibf{{\boldsymbol \xi}}
\def\taubf{{\boldsymbol \tau}}
\def\sigmahat{{\widehat{\sigma}}}
\def\thetabf{{\bm{\theta}}}
\def\thetabfhat{{\widehat{\bm{\theta}}}}
\def\thetahat{{\widehat{\theta}}}
\def\mubar{\overline{\mu}}
\def\muavg{\mu}
\def\sigbf{\bm{\sigma}}
\def\etal{\emph{et al.}}
\def\Ggothic{\mathfrak{G}}
\def\Pset{{\mathcal P}}
\newcommand{\bigCond}[2]{\bigl({#1} \!\bigm\vert\! {#2} \bigr)}
\newcommand{\BigCond}[2]{\Bigl({#1} \!\Bigm\vert\! {#2} \Bigr)}
\newcommand{\tran}{^{\text{\sf T}}}
\newcommand{\herm}{^{\text{\sf H}}}
\newcommand{\bkt}[1]{{\langle #1 \rangle}}
\def\Norm{{\mathcal N}}
\newcommand{\vmult}{.}
\newcommand{\vdiv}{./}


% Python style for highlighting
\newcommand\pythonstyle{\lstset{
language=Python,
backgroundcolor=\color{backcolour},
commentstyle=\color{deepgreen},
basicstyle=\ttm,
otherkeywords={self},             % Add keywords here
keywordstyle=\ttb\color{deepblue},
emph={MyClass,__init__},          % Custom highlighting
emphstyle=\ttb\color{deepred},    % Custom highlighting style
stringstyle=\color{deepgreen},
%frame=tb,                         % Any extra options here
showstringspaces=false            %
}}

% Python environment
\lstnewenvironment{python}[1][]
{
\pythonstyle
\lstset{#1}
}
{}

% Python for external files
\newcommand\pythonexternal[2][]{{
\pythonstyle
\lstinputlisting[#1]{#2}}}

% Python for inline
\newcommand\pycode[1]{{\pythonstyle\lstinline!#1!}}

\begin{document}

\title{Introduction to Machine Learning\\
Homework 10: Clustering: K-means and EM-GMM algorithm \\ Solutions}
\author{Prof. Yao Wang}
\date{}

\maketitle

\begin{enumerate}

\item Figure 1 shows a set of samples to be clustered. Show the results from K-means algorithm in successive iterations, starting with the initial centroids indicated in the figure. You can do nearest neighbor partition and centroid update approximately by ``eyeballing''. 

Solution: See the figure below. After step 3a, the partition is the same as in step 2a. So the algorithm converged.

\begin{figure*}[h]
\centering
\includegraphics[width=\columnwidth]{p1_solution.png}
\caption{Illustration of successive K-means iterations.} \label{fig:samples}
\end{figure*}

 
 \item\label{kmeans-NN}
 Suppose you have conducted a clustering analysis for a dataset with each sample described by $D$ features, and you used K-means algorithm to derive $K$ clusters and determined the cluster model parameters (including the centroids of the $K$ clusters). Given a test dataset containing $N$ samples,  you want to classify each sample into one of the cluster using the nearest neighbor rule. How many computations are needed? For simplicity, for this and all following problems, only count multiplications (consider the square operation as  multiplication).

 
 Solution: For each sample $x$, we need to compare it with each of the $K$ cluster centroids $\mu_k$ by computing the distance square $d(x, \mu_k)^2=\|x-\mu_k\|^2 =\sum_{d=1}^D (x_d-\mu_{k,d})^2$. We need to compute the distance with all $K$ centroids and finding the one with the minimal distance. Each distance calculation needs $D$ multiplications and $D-1$ additions. Comparing each pixel with $K$ centroids requires $KD$ multiplications and $K(D-1)$ additions. Repeating this for  $N$ pixels requires $NK D$ multiplications and $NK (D-1)$ additions. Overall, we can say the complexity is $O(NKD).$
 
\item\label{kmeans}
Suppose you are given $N$ samples each described by $D$ features, and you are asked to cluster them into $K$ clusters using the K-means algorithm. Suppose you run the K-means iteration $T$ times. How many computations are needed? 

Solution: In each iteration, we first need to  do a nearest neighbor search for each sample. This requires the same amount of computation as in the previous problem: $NKD$ multiplications and $NK(D-1)$ additions. Then we need to recompute the centroid of each cluster. If a cluster has $N_k$ samples assigned to it, computing the mean requires $D(N_k-1)$ additions and $D$ divisions. Computing the centroids for all $K$ clusters requires $D(N-K)$ additions and $DK$ divisions. This is negligible compared to the first step for nearest neighbor search.  Therefore, the total computation in each iteration is $O(NKD)$.  $T$ iterations would take $O(TNKD).$

  
 \item (Optional)
  Suppose you have conducted a clustering analysis for a dataset with each sample described by $D$ features, and you used EM-GMM algorithm to derive $K$ clusters and determined the cluster model parameters (including the prior probabilities, centroids and covariance matrices of the $K$ clusters). Given a test dataset containing $N$ samples,  you want to classify each sample into the cluster  that has the highest posterior probability. How many computations are needed? 
 
 
 Solution: For each sample  $x_n$, we need to compute the posterior probability $\gamma_{n,i}$ that it belongs to cluster $i$ for all $K$ clusters, and find the cluster that has the highest probability.   Recall that this probability can be expressed as 
 $$\gamma_{n,i} = \frac{q_i {\cal N}(x_n|\mu_i,P_i) }{\sum_{k=1}^K q_k {\cal N}(x_n|\mu_k,P_k)} $$
 Therefore, we need to compute $q_k {\cal N}(x_n|\mu_k,P_k), \forall k\in {1,2,\ldots,K}$.
 Recall that 
 $${\cal N}(x_n|\mu_k,P_k) = \frac{1}{(2\pi)^{D/2} | P_k|^{1/2}} \exp \left \{ -\frac{1}{2} (x_n-\mu_k)^T P_k^{-1} (x_n-\mu_k) \right \}.$$
 Given the GMM model, $q_k, \mu_k, P_k$ are constants, so we mainly need to compute the distance in the exponent $(x_n-\mu_k)^T P_k^{-1} (x_n-\mu_k)$. If the covariance matrix $P_k$ and hence $P_k^{-1}$ is a full matrix (of dimension $D\times D$), we first need to compute $z=P_k^{-1} (x_n-\mu_k)$. This requires $D^2$ multiplications (we will ignore the additions). Then we need to compute  $(x_n-\mu_k)^T z$, which requires $D$ multiplications. Ignore all other operations, computing $q_k {\cal N}(x_n|\mu_k,P_k) $ for each $k$ requires about $D^2+D$ multiplications. Computing this for all $K$ clusters and consequently $\gamma_{n,i}$ for all $i \in {1,2,\ldots, K}$ requires $K(D^2+D)$ multiplications. Repeating this all for samples will require $N K (D^2+D)$ multiplications. Overall, we can say the complexity is  $O(NKD^2).$ Compared to k-means (Prob.~\ref{kmeans-NN}), it is at least $D$ times more computations.
 
 
 
 \item (Optional)
Suppose you are given $N$ samples each described by $D$ features, and you are asked to cluster them into $K$ clusters using the EM-GMM algorithm. Suppose you run the EM  iteration $T$ times. How many computations are needed? 



Solution: In each iteration, we first need to  compute the posterior probability $\gamma_{n,i}$ based on the model parameters from the previous iteration. This will require  $O(NKD^2)$ computations, as shown in the previous problem. Then we need to update the $q_k, \mu_k,$ and $P_k$. Computing $q_k$ requires only addition. Computing $\mu_k= \sum_{n} \gamma_{n,k} x_n $ requires $N$ multiplications. Computing $P_k=\sum_n \gamma_{n,k} (x_n-\mu_k) (x_n-\mu_k)^{T}$ requires $N (D^2+1)$ multiplications. Computing these parameters for all $K$ clusters take on the order of $ K N (D^2+2)$ operations. So each iteration takes  $O(NKD^2)$ operations. $T$ iterations will thus take $O(TNKD^2)$ operations. Again, this is about $D$ times more than the k-means algorithms (Prob.~\ref{kmeans}).

\end{enumerate}
  \end{document}
